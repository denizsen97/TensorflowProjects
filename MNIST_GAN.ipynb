{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "Question_2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWiLh0_3jXcp",
        "colab_type": "text"
      },
      "source": [
        "# Assignment 3 Question 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlaPZjSmjXcs",
        "colab_type": "text"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "This tutorial demonstrates the basic workflow of using TensorFlow with a simple linear model. After loading the so-called MNIST data-set with images of hand-written digits, we define and train a simple mathematical model in TensorFlow. The results are then plotted and discussed.\n",
        "\n",
        "You should be familiar with basic linear algebra, Python and the Jupyter Notebook editor. It also helps if you have a basic understanding of Machine Learning and classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aftm6IOkjXct",
        "colab_type": "text"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgfCE2czjXcu",
        "colab_type": "code",
        "outputId": "dc40badc-0731-484f-e5ef-df5676cc1930",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "%tensorflow_version 1.x\" before \"import tensorflow\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `1.x\" before \"import tensorflow`. This will be interpreted as: `1.x`.\n",
            "\n",
            "\n",
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaQOak0ujXcz",
        "colab_type": "text"
      },
      "source": [
        "Check TensorFlow version:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzm9w2rfjXc0",
        "colab_type": "code",
        "outputId": "4d8b119a-2d74-4c70-997b-7ae1f6a19196",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.15.2'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnostdBUjXc4",
        "colab_type": "text"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eU_ttPJHjXc5",
        "colab_type": "text"
      },
      "source": [
        "The MNIST data-set is about 12 MB and will be downloaded automatically if it is not located in the given path."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znKeG0KLjXc6",
        "colab_type": "code",
        "outputId": "73e91cec-7fa1-492e-890b-37678b23d9ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "source": [
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "data = input_data.read_data_sets(\"data/MNIST/\", one_hot=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-3-d748032e7ab8>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting data/MNIST/train-images-idx3-ubyte.gz\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting data/MNIST/train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Extracting data/MNIST/t10k-images-idx3-ubyte.gz\n",
            "Extracting data/MNIST/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmLNHo1KjXdB",
        "colab_type": "text"
      },
      "source": [
        "The MNIST data-set has now been loaded and consists of 70.000 images and associated labels (i.e. classifications of the images). The data-set is split into 3 mutually exclusive sub-sets. We will only use the training and test-sets in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-U6PJadEjXdD",
        "colab_type": "code",
        "outputId": "351a30ae-8486-476c-f6da-8a05617e6120",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(data.train.images) # Train image count"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "55000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-sZC3NdjXdH",
        "colab_type": "code",
        "outputId": "6f6f073c-7305-42eb-af7b-ff5f3357784f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(data.test.images) # Test image count"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vFvluxBjXdK",
        "colab_type": "text"
      },
      "source": [
        "### One-Hot Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56yLnLfbjXdL",
        "colab_type": "text"
      },
      "source": [
        "The data-set has been loaded as so-called One-Hot encoding. This means the labels have been converted from a single number to a vector whose length equals the number of possible classes. All elements of the vector are zero except for the $i$'th element which is one and means the class is $i$. For example, the One-Hot encoded labels for the first 5 images in the test-set are:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPiK8jhEjXdM",
        "colab_type": "code",
        "outputId": "1b39931c-1179-4453-ac23-975d10850921",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "data.test.labels[0:5, :]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wJ7yT34jXdS",
        "colab_type": "text"
      },
      "source": [
        "We also need the classes as single numbers for various comparisons and performance measures, so we convert the One-Hot encoded vectors to a single number by taking the index of the highest element. Note that the word 'class' is a keyword used in Python so we need to use the name 'cls' instead."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3KYBKT8jXdT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.test.cls = np.array([label.argmax() for label in data.test.labels])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvOf43uwjXdd",
        "colab_type": "text"
      },
      "source": [
        "We can now see the class for the first five images in the test-set. Compare these to the One-Hot encoded vectors above. For example, the class for the first image is 7, which corresponds to a One-Hot encoded vector where all elements are zero except for the element with index 7."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePm97LERjXdd",
        "colab_type": "code",
        "outputId": "cdd7fea2-3cea-4af8-cf60-4bc400935ccc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data.test.cls[0:5]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([7, 2, 1, 0, 4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2X8Kz8o6jXdg",
        "colab_type": "text"
      },
      "source": [
        "### Data dimensions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQEJmptijXdg",
        "colab_type": "text"
      },
      "source": [
        "The data dimensions are used in several places in the source-code below. In computer programming it is generally best to use variables and constants rather than having to hard-code specific numbers every time that number is used. This means the numbers only have to be changed in one single place. Ideally these would be inferred from the data that has been read, but here we just write the numbers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcbxxBpAjXdh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We know that MNIST images are 28 pixels in each dimension.\n",
        "img_size = 28\n",
        "\n",
        "# Images are stored in one-dimensional arrays of this length.\n",
        "img_size_flat = img_size * img_size\n",
        "\n",
        "# Tuple with height and width of images used to reshape arrays.\n",
        "img_shape = (img_size, img_size)\n",
        "\n",
        "# Number of classes, one class for each of 10 digits.\n",
        "num_classes = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRaEiAIAjXdn",
        "colab_type": "text"
      },
      "source": [
        "### Plot a few images to see if data is correct"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1e52xJejXdo",
        "colab_type": "text"
      },
      "source": [
        "All images are 1D. We need to reshape them to form a 2D image matrix. There are 55000 train samples, each has 784 pixels. We have to reshape images to (28,28)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1CIzEztjXdp",
        "colab_type": "code",
        "outputId": "e54dacb1-17e7-4022-ec95-ca3b1d0b581d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Sample to plot\n",
        "sample = 0\n",
        "\n",
        "# Reshape sample from 784 to 28x28\n",
        "reshaped_sample = np.reshape(data.train.images[sample], newshape=(28,28)) \n",
        "print(reshaped_sample)\n",
        "# Plot reshaped sample image using grayscale color map\n",
        "plt.imshow(reshaped_sample, cmap=\"gray\") \n",
        "\n",
        "# Print true sample class\n",
        "print (data.train.labels[sample]) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.3803922\n",
            "  0.37647063 0.3019608  0.46274513 0.2392157  0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.3529412  0.5411765  0.9215687\n",
            "  0.9215687  0.9215687  0.9215687  0.9215687  0.9215687  0.9843138\n",
            "  0.9843138  0.9725491  0.9960785  0.9607844  0.9215687  0.74509805\n",
            "  0.08235294 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.54901963 0.9843138  0.9960785  0.9960785\n",
            "  0.9960785  0.9960785  0.9960785  0.9960785  0.9960785  0.9960785\n",
            "  0.9960785  0.9960785  0.9960785  0.9960785  0.9960785  0.9960785\n",
            "  0.7411765  0.09019608 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.8862746  0.9960785  0.81568635 0.7803922\n",
            "  0.7803922  0.7803922  0.7803922  0.54509807 0.2392157  0.2392157\n",
            "  0.2392157  0.2392157  0.2392157  0.5019608  0.8705883  0.9960785\n",
            "  0.9960785  0.7411765  0.08235294 0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.14901961 0.32156864 0.0509804  0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.13333334 0.8352942\n",
            "  0.9960785  0.9960785  0.45098042 0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.32941177\n",
            "  0.9960785  0.9960785  0.9176471  0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.32941177\n",
            "  0.9960785  0.9960785  0.9176471  0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.4156863  0.6156863\n",
            "  0.9960785  0.9960785  0.95294124 0.20000002 0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.09803922\n",
            "  0.45882356 0.8941177  0.8941177  0.8941177  0.9921569  0.9960785\n",
            "  0.9960785  0.9960785  0.9960785  0.94117653 0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.26666668 0.4666667  0.86274517\n",
            "  0.9960785  0.9960785  0.9960785  0.9960785  0.9960785  0.9960785\n",
            "  0.9960785  0.9960785  0.9960785  0.5568628  0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.14509805 0.73333335 0.9921569  0.9960785  0.9960785\n",
            "  0.9960785  0.8745099  0.8078432  0.8078432  0.29411766 0.26666668\n",
            "  0.8431373  0.9960785  0.9960785  0.45882356 0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.4431373  0.8588236  0.9960785  0.9490197  0.89019614 0.45098042\n",
            "  0.34901962 0.12156864 0.         0.         0.         0.\n",
            "  0.7843138  0.9960785  0.9450981  0.16078432 0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.6627451  0.9960785  0.6901961  0.24313727 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.18823531\n",
            "  0.9058824  0.9960785  0.9176471  0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.07058824 0.48627454 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.32941177\n",
            "  0.9960785  0.9960785  0.6509804  0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.54509807\n",
            "  0.9960785  0.9333334  0.22352943 0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.8235295  0.9803922\n",
            "  0.9960785  0.65882355 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.9490197  0.9960785\n",
            "  0.93725497 0.22352943 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.34901962 0.9843138  0.9450981\n",
            "  0.3372549  0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.01960784 0.8078432  0.96470594 0.6156863\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.01568628 0.45882356 0.27058825 0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]]\n",
            "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxqdmE6UjXdv",
        "colab_type": "text"
      },
      "source": [
        "## TensorFlow Graph\n",
        "\n",
        "The entire purpose of TensorFlow is to have a so-called computational graph that can be executed much more efficiently than if the same calculations were to be performed directly in Python. TensorFlow can be more efficient than NumPy because TensorFlow knows the entire computation graph that must be executed, while NumPy only knows the computation of a single mathematical operation at a time.\n",
        "\n",
        "TensorFlow can also automatically calculate the gradients that are needed to optimize the variables of the graph so as to make the model perform better. This is because the graph is a combination of simple mathematical expressions so the gradient of the entire graph can be calculated using the chain-rule for derivatives.\n",
        "\n",
        "TensorFlow can also take advantage of multi-core CPUs as well as GPUs - and Google has even built special chips just for TensorFlow which are called TPUs (Tensor Processing Units) and are even faster than GPUs.\n",
        "\n",
        "A TensorFlow graph consists of the following parts which will be detailed below:\n",
        "\n",
        "* Placeholder variables used to change the input to the graph.\n",
        "* Model variables that are going to be optimized so as to make the model perform better.\n",
        "* The model which is essentially just a mathematical function that calculates some output given the input in the placeholder variables and the model variables.\n",
        "* A cost measure that can be used to guide the optimization of the variables.\n",
        "* An optimization method which updates the variables of the model.\n",
        "\n",
        "In addition, the TensorFlow graph may also contain various debugging statements e.g. for logging data to be displayed using TensorBoard, which is not covered in this tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Jhl8-yljXdw",
        "colab_type": "text"
      },
      "source": [
        "### Placeholder variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBEAn35GFBeq",
        "colab_type": "text"
      },
      "source": [
        "Here, we are defining the placeholder variables to which we will feed the inputs to the network. We firstly have **x_input** that will hold the real images from the data set, and its dimensions are None(batch size) and flattened image size. **z_input** will be fed with the noise vector which will be used for the image generation and its size is None and 100 which is the length of the noise vector. **dropout** is the keep probability of for the dropout operations, altough it is not used in the final version of the code therefore it can be ignored."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dO068srvjXdz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tf.variable_scope('D'):\n",
        "  x_input = tf.placeholder(tf.float32, [None, img_size_flat], name=\"x_input\")\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6Mh9-yIjXd4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tf.variable_scope('G'):\n",
        "  z_input = tf.placeholder(tf.float32, [None, 100], name=\"z_input\")\n",
        "\n",
        "\n",
        "dropout = tf.placeholder(tf.float32, name=\"dropout\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJABDNhajXeF",
        "colab_type": "text"
      },
      "source": [
        "### Variables to be optimized"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYRmXRo6jXeF",
        "colab_type": "text"
      },
      "source": [
        "Apart from the placeholder variables that were defined above and which serve as feeding input data into the model, there are also some model variables that must be changed by TensorFlow so as to make the model perform better on the training data.\n",
        "\n",
        "We define the variables inside a dictionnary for easier access. The variables are created with tf.get_variable and initialized with Xavier initialization method for better performance. Both generator and the discriminators are fully connected networks with a single hiden layer with 128 units and an output layer. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15lKq1zrjXeG",
        "colab_type": "code",
        "outputId": "16551ecc-0b95-485e-909f-aa8c6345bda0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\n",
        "weights = {\n",
        "    'g_fc' : tf.get_variable(\"DenseHiddenGenerator\", [100, 128], initializer=tf.glorot_normal_initializer()),\n",
        "    'g_out' : tf.get_variable(\"DenseOutGenerator\", [128, img_size_flat], initializer=tf.glorot_normal_initializer()),\n",
        "    'd_fc' : tf.get_variable(\"DenseHiddenDiscriminator\", [img_size_flat, 128], initializer=tf.glorot_normal_initializer()),\n",
        "    'd_out' : tf.get_variable(\"DenseOutDiscriminator\", [128, 1], initializer=tf.glorot_normal_initializer())\n",
        "}\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nweights = {\\n    \\'g_fc\\' : tf.Variable( tf.truncated_normal([100, 128]), name= \"DenseHiddenGenerator\"),\\n    \\'g_out\\' : tf.Variable( tf.truncated_normal([128, img_size_flat]), name= \"DenseOutGenerator\"),\\n    \\'d_fc\\' : tf.Variable( tf.truncated_normal([img_size_flat, 128]), name= \"DenseHiddenDiscriminator\"),\\n    \\'d_out\\' : tf.Variable( tf.truncated_normal([128, 1]), name= \"DenseOutDiscriminator\")\\n}\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cwdkR8VjXeI",
        "colab_type": "text"
      },
      "source": [
        "We also define a dictionnary for the bias variables, which are added to the outputs of the multiplications on the layers. These variables are initialized with constant initializer which by default sets the values to 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UM-ydEIRjXeJ",
        "colab_type": "code",
        "outputId": "b3036fa1-cbad-415b-e398-69b8cb72ffe3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\n",
        "biases = {\n",
        "      'g_fc' : tf.get_variable(\"BiasHiddenGenerator\", [128], initializer=tf.constant_initializer()),\n",
        "      'g_out' : tf.get_variable(\"BiasOutGenerator\", [img_size_flat], initializer=tf.constant_initializer()),\n",
        "      'd_fc' : tf.get_variable(\"BiasHiddenDiscriminator\", [128], initializer=tf.constant_initializer()),\n",
        "      'd_out' : tf.get_variable(\"BiasOutDiscriminator\", [1], initializer=tf.constant_initializer())\n",
        "}\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n\\nbiases = {\\n    \\'g_fc\\' : tf.Variable( tf.zeros(128), name= \"DenseHiddenGenerator\"),\\n    \\'g_out\\' : tf.Variable( tf.zeros(img_size_flat), name= \"DenseOutGenerator\"),\\n    \\'d_fc\\' : tf.Variable( tf.zeros(128), name= \"DenseOutDiscriminator\"),\\n    \\'d_out\\' : tf.Variable( tf.zeros(1), name= \"DenseOutDiscriminator\"),\\n}\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXNKzQ5KjXeL",
        "colab_type": "text"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URLuLQy6jXeM",
        "colab_type": "text"
      },
      "source": [
        "Our model is constituted by 2 networks; generator and discriminator. The generator takes the noise vector of size 100 as input and produces a flattened image with size 748. It uses relu activation in the hidden layer and sigmoid in the output layer. \n",
        "\n",
        "Also the forward pass functions are separated from the definitions of the vaariables. This is because the generator forward pass is also used in the forward pass of the discriminator. Therefore to avoid redefinitions, I have separated these parts. I have also separated the forward pass function of the discriminator.   \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MV72Z0E4jXeM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#generator variables\n",
        "with tf.variable_scope('G'):\n",
        "  g_hidden = weights['g_fc']\n",
        "  g_out = weights['g_out']\n",
        "  g_hidden_bias = biases['g_fc']\n",
        "  g_out_bias = biases['g_out']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqhD1VK3IsZE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generator_forward(x):\n",
        "  x = tf.add(tf.matmul(x, g_hidden), g_hidden_bias)\n",
        "  x = tf.nn.relu(x)\n",
        "\n",
        "  # The dropout operation is not used in this implementation as it seems to diverge the losses much faster\n",
        "  #x = tf.nn.dropout(x, dropout)\n",
        "  logits = tf.add(tf.matmul(x, g_out), g_out_bias)\n",
        "  x = tf.nn.sigmoid(logits)\n",
        "  return x, logits\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGPDIDFVAaOy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#discriminator variables\n",
        "with tf.variable_scope('D'):\n",
        "  d_hidden = weights['d_fc']\n",
        "  d_out = weights['d_out']\n",
        "  d_hidden_bias = biases['d_fc']\n",
        "  d_out_bias = biases['d_out']\n",
        "\n",
        "def discriminator_forward(x):\n",
        "  x = tf.add(tf.matmul(x, d_hidden), d_hidden_bias)\n",
        "  x = tf.nn.relu(x)\n",
        "  #x = tf.nn.dropout(x, dropout)\n",
        "\n",
        "  logits = tf.add(tf.matmul(x, d_out), d_out_bias)\n",
        "  x = tf.nn.sigmoid(logits)\n",
        "  return x, logits\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_U_tVxtIjXeO",
        "colab_type": "text"
      },
      "source": [
        "The forward pass of the generator gives both the logits(*gen_out_logit*) and the outputs of the sigmoid function(*gen_out_sigmoid*). The activation output is the real image output therefore this is fed to the discriminator for it to evaluate this image. z_input palceholder is fed into the forward function to generate image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14rwPJoJjXeP",
        "colab_type": "code",
        "outputId": "47892a2b-c306-4595-da7e-d050901c47fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "with tf.variable_scope('G'):\n",
        "  gen_out_sigmoid, gen_out_logit = generator_forward(z_input)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-15-d9952698d05e>:14: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMwnGKSGKeyD",
        "colab_type": "text"
      },
      "source": [
        "The outputs of the discriminator are in the same way as the generator; altough the outputs of the activation are only used for testing purposes and for now they are redundant. *gen_out_sigmoid*, which is the generated image, is fed into the first pass of the discriminator. In the second pass, the *x_input* is fed as we also need to train the network to learn about the real images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYrETxn4jXeS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tf.variable_scope('D'):\n",
        "  disc_fake_out_sigmoid, disc_fake_out_logit = discriminator_forward(gen_out_sigmoid)\n",
        "  disc_real_out_sigmoid, disc_real_out_logit = discriminator_forward(x_input)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmtvMLAbjXeW",
        "colab_type": "text"
      },
      "source": [
        "### Cost-function to be optimized"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnZ_Ci78jXeX",
        "colab_type": "text"
      },
      "source": [
        "To make the discriminator be better at classifying real and fake images, and make generator be better at generating realistic images, we define separate cost functions. We are using cross entropy as the loss functions. \n",
        "\n",
        "The loss function of the discriminator is the sum of two losses where the expectations are as follows: classify real images as real(0) and classify fake images as fake(1).\n",
        "\n",
        "Contrarily, the generator tries to incraese the loss of the discriminator by aiming to make discriminator classify fake images as real(0)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VT3paCT8jXeY",
        "colab_type": "code",
        "outputId": "33f740b4-b122-49e4-df03-ded35ab3af2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "#real is 0 and fake is 1\n",
        "d_loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=disc_real_out_logit, labels=tf.zeros_like(disc_real_out_logit))  + tf.nn.sigmoid_cross_entropy_with_logits(logits=disc_fake_out_logit, labels=tf.ones_like(disc_fake_out_logit))\n",
        "\n",
        "g_loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=disc_fake_out_logit, labels=tf.zeros_like(disc_fake_out_logit)) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNoDenzNjXed",
        "colab_type": "text"
      },
      "source": [
        "As we calculate the cross entropies for each image, we need to get a two loss values. This is done by separately averaging the contents of **d_loss** and **g_loss**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xup-z-ptll85",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d_cost = tf.reduce_mean(d_loss)\n",
        "g_cost = tf.reduce_mean(g_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6Y6vYceNHLy",
        "colab_type": "text"
      },
      "source": [
        "We need to separate the weights of the 2 networks to easily update them during the training. We have put 'Discriminator' and 'Generator' prefices to the weights and biases depending on their networks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVQu5hR2jXee",
        "colab_type": "code",
        "outputId": "1714643f-3867-40e5-e375-20eb926f19e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "disc_vars = [var for var in tf.trainable_variables() if 'Discriminator' in var.name]\n",
        "gen_vars = [var for var in tf.trainable_variables() if 'Generator' in var.name]\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[<tf.Variable 'DenseHiddenDiscriminator:0' shape=(784, 128) dtype=float32_ref>, <tf.Variable 'DenseOutDiscriminator:0' shape=(128, 1) dtype=float32_ref>, <tf.Variable 'BiasHiddenDiscriminator:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'BiasOutDiscriminator:0' shape=(1,) dtype=float32_ref>]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccmvl0_FjXeh",
        "colab_type": "text"
      },
      "source": [
        "### Optimization method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIcFXeHTjXei",
        "colab_type": "text"
      },
      "source": [
        "We create two Adam optimizers for the networks and set their variable lists as the previously separated lists. The learning rates are initialized with 0.0001."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mV0BOxmfjXej",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "D_LR = 0.0001\n",
        "G_LR = 0.0001\n",
        "\n",
        "d_optimizer = tf.train.AdamOptimizer(learning_rate=D_LR).minimize(d_cost, var_list=disc_vars)\n",
        "g_optimizer = tf.train.AdamOptimizer(learning_rate=G_LR).minimize(g_cost, var_list=gen_vars)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWGoiEY0jXe0",
        "colab_type": "text"
      },
      "source": [
        "## TensorFlow Run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzYKacsEjXe2",
        "colab_type": "text"
      },
      "source": [
        "### Create TensorFlow session\n",
        "\n",
        "Once the TensorFlow graph has been created, we have to create a TensorFlow session which is used to execute the graph."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Oh2AcwFjXe3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "session = tf.Session()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NerKywC_jXe8",
        "colab_type": "text"
      },
      "source": [
        "### Initialize variables\n",
        "\n",
        "The variables for `weights` and `biases` must be initialized before we start optimizing them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIJqYUobjXe9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "session.run(tf.global_variables_initializer())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVncFmcojXfB",
        "colab_type": "text"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wC5fsyzjXfC",
        "colab_type": "text"
      },
      "source": [
        "There are 55.000 images in the training-set. It takes a long time to calculate the gradient of the model using all these images. We therefore use a small batch of images in each iteration of the optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27EeQBKAjXfH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vppv0W9jXfS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "import os\n",
        "\n",
        "if not os.path.exists('output'):\n",
        "    os.makedirs('output')\n",
        "'''\n",
        "\n",
        "d_loss_epoch = []\n",
        "g_loss_epoch = []\n",
        "total_num_epochs = 0\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzGMYGejOMFn",
        "colab_type": "text"
      },
      "source": [
        "Each training iteration is constituted by the training of the discriminator and the generator. We generate a noise vector for the discriminator to feed to the generator to get generated images. These images and the trainin images are then used to train the discriminator. \n",
        "\n",
        "Then we train the generator by generating another noise vector and feed it. The generator generates images and feeds them to the discriminator to get a prediction and the optimizer uses this prediction to update the weigths of the generator.\n",
        "\n",
        "Every 10 epochs, the network shows outputs from the generator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CuyNJG_Q7I6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from time import time\n",
        "\n",
        "def train(num_epochs):\n",
        "    global total_num_epochs\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "      total_num_epochs = total_num_epochs + 1\n",
        "      g_loss_iter = []\n",
        "      d_loss_iter = []\n",
        "      start_time = time()\n",
        "      \n",
        "      for i in range(len(data.train.images)//batch_size):\n",
        "          x_batch, y_true_batch = data.train.next_batch(batch_size, shuffle=False)\n",
        "\n",
        "          z_batch = np.random.normal(50, 100, (batch_size, 100)).astype('f')\n",
        "          _, discriminator_loss, d_real_output, d_fake_output = session.run([d_optimizer, d_cost, disc_real_out_sigmoid, disc_fake_out_sigmoid], feed_dict={x_input: x_batch, z_input: z_batch, dropout:0.7})\n",
        "\n",
        "          z_batch = np.random.normal(50, 100, (batch_size, 100)).astype('f')\n",
        "          _, generator_loss = session.run([g_optimizer, g_cost], feed_dict={z_input: z_batch, dropout:0.7})\n",
        "\n",
        "          d_loss_iter.append(discriminator_loss)\n",
        "          g_loss_iter.append(generator_loss)\n",
        "      d_mean_loss = np.mean(d_loss_iter)\n",
        "      g_mean_loss = np.mean(g_loss_iter)\n",
        "      print(\"Epoch {} -> Discriminator loss:{} Generator Loss:{} Elapsed time:{}s \".format(total_num_epochs, d_mean_loss, g_mean_loss, time()-start_time))\n",
        "      d_loss_epoch.append(d_mean_loss)\n",
        "      g_loss_epoch.append(g_mean_loss)\n",
        "\n",
        "      if total_num_epochs % 10 == 0:\n",
        "        z_batch = np.random.normal(50, 100, (9, 100)).astype('f')\n",
        "        gen_images = session.run(gen_out_sigmoid, feed_dict={z_input: z_batch, dropout:1.0})\n",
        "        fig=plt.figure(figsize=(10, 10))\n",
        "        columns = 3\n",
        "        rows = 3\n",
        "        for t in range(1, columns*rows +1):\n",
        "            fig.add_subplot(rows, columns, t)\n",
        "            #plt.imshow(np.reshape(gen_images[i-1], newshape=(img_size, img_size)), cmap=\"gray\")\n",
        "            plt.imshow(np.reshape(gen_images[t-1], newshape=(img_size, img_size)), cmap=\"gray\")\n",
        "              \n",
        "              \n",
        "        #plt.savefig(\"output/{}_output.png\".format(str(total_num_epochs)))\n",
        "        plt.show()\n",
        "          \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gfTON3tjXgA",
        "colab_type": "text"
      },
      "source": [
        "## Performance after 10 training epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58WvfY-djXgB",
        "colab_type": "code",
        "outputId": "303ff8bf-171b-4039-f936-a8bddb9b5126",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        }
      },
      "source": [
        "train(10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-29-42959974f55f>:2: simple_save (from tensorflow.python.saved_model.simple_save) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.simple_save.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-42959974f55f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#save the model to be reloaded when needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimple_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content/output/model\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"x_input\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"z_input\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mz_input\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"output\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgen_out_sigmoid\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m               \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m               instructions)\n\u001b[0;32m--> 324\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[1;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/saved_model/simple_save.py\u001b[0m in \u001b[0;36msimple_save\u001b[0;34m(session, export_dir, inputs, outputs, legacy_init_op)\u001b[0m\n\u001b[1;32m     81\u001b[0m           \u001b[0msignature_def_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_signature_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m   }\n\u001b[0;32m---> 83\u001b[0;31m   \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSavedModelBuilder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexport_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m   b.add_meta_graph_and_variables(\n\u001b[1;32m     85\u001b[0m       \u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/saved_model/builder_impl.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, export_dir)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexport_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSavedModelBuilder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexport_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexport_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_add_collections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0massets_collection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/saved_model/builder_impl.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, export_dir)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;34m\"Export directory already exists, and isn't empty. Please choose \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0;34m\"a different export directory, or delete all the contents of the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \"specified directory: %s\" % export_dir)\n\u001b[0m\u001b[1;32m    104\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m       \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecursive_create_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_export_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: Export directory already exists, and isn't empty. Please choose a different export directory, or delete all the contents of the specified directory: content/output/model"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5AY3khnjXgJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRC658mwjXgL",
        "colab_type": "text"
      },
      "source": [
        "## Performance after 300 training iterations\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVyT1Ps8jXgM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train(290)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N19zaIajjXgT",
        "colab_type": "text"
      },
      "source": [
        "## Loss plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_gPPUqhjXgU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = np.linspace(1, len(d_loss_epoch), len(d_loss_epoch))\n",
        "fig = plt.figure()\n",
        "plt.plot(x, d_loss_epoch, 'r', label=\"Discriminator\")\n",
        "plt.plot(x, g_loss_epoch, 'b', label=\"Generator\")\n",
        "plt.legend()\n",
        "plt.title(\"Losses of 2 networks\")\n",
        "plt.show()\n",
        "fig.savefig(\"output/loss_graph.png\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99apeuv5jXgX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#zip the contents of the output directory\n",
        "#!zip -r /content/file.zip /content/output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swkABudOjXgc",
        "colab_type": "text"
      },
      "source": [
        "We are now done using TensorFlow, so we close the session to release its resources."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPFvovP6jXgc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "session.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1q0FnB6hjXgf",
        "colab_type": "text"
      },
      "source": [
        "## References\n",
        "\n",
        "[1] [Magnus Erik Hvass Pedersen](http://www.hvass-labs.org/)"
      ]
    }
  ]
}